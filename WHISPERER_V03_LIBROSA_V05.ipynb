{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/openai/whisper-large-v3\n",
    "\n",
    "https://huggingface.co/docs/transformers/en/model_doc/whisper\n",
    "\n",
    "https://stackoverflow.com/questions/73822353/how-can-i-get-word-level-timestamps-in-openais-whisper-asr\n",
    "\n",
    "https://community.openai.com/t/how-to-get-whispers-api-to-add-timestamps-to-the-transcripts/501788/3 \n",
    "\n",
    "\n",
    "Whisper\n",
    "\n",
    "Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. \n",
    "\n",
    "Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning.\n",
    "\n",
    "Whisper was proposed in the paper Robust Speech Recognition via Large-Scale Weak Supervision by Alec Radford et al. from OpenAI. \n",
    "The original code repository can be found here. https://github.com/openai/whisper"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### INSTALL requirements\n",
    "\n",
    "FFMEG INSTALL separately:  brew install ffmpeg MACOS | winget install https://www.gyan.dev/ffmpeg/builds/\n",
    "\n",
    "!pip install --upgrade pip\n",
    "\n",
    "!pip install --upgrade git+https://github.com/huggingface/transformers.git accelerate datasets[audio]\n",
    "\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "from soundfile import read\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float16 # change to \"int8\" if low on GPU mem (may reduce accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch_dtype, \n",
    "    low_cpu_mem_usage=False, \n",
    "    #attn_implementation=\"flash_attention_2\", # UNIX only: !pip install flash-attn --no-build-isolation\n",
    "    use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=32, # reduce if low on GPU mem\n",
    "    return_timestamps=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/Willi/Documents/code/HUGGINGFACE_WISHPERER/AUDIO/harvard.wav']\n"
     ]
    }
   ],
   "source": [
    "# Get list of wav files\n",
    "#wav_files = glob.glob(\"./intakes/*.wav\")\n",
    "wav_files = glob.glob(\"/Users/Willi/Documents/code/HUGGINGFACE_WISHPERER/AUDIO/*.wav\")\n",
    "print(wav_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(wav_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_dataframe(res):\n",
    "  \"\"\"\n",
    "  This function creates a Pandas DataFrame from a list named 'res'.\n",
    "\n",
    "  Args:\n",
    "      res: A list of dictionaries containing 'timestamp' and 'text' keys.\n",
    "\n",
    "  Returns:\n",
    "      A Pandas DataFrame with three columns: 'start_time', 'end_time', and 'text'.\n",
    "  \"\"\"\n",
    "\n",
    "  # Create lists of timestamps and text from the 'res' list\n",
    "  timestamps = [item['timestamp'] for item in res]\n",
    "  text = [item['text'] for item in res]\n",
    "\n",
    "  # Extract start and end time from each timestamp tuple\n",
    "  start_time = [t[0] for t in timestamps]\n",
    "  end_time = [t[1] for t in timestamps]\n",
    " \n",
    "\n",
    "  # Create a DataFrame with separate columns\n",
    "  data = pd.DataFrame({'start_time': start_time, 'end_time': end_time, 'text': text})\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/mambaforge/base/envs/STT-env/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:509: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "You have passed language=dutch, but also have set `forced_decoder_ids` to [[1, None], [2, 50360]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of language=dutch.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file 1 (/Users/Willi/Documents/code/HUGGINGFACE_WISHPERER/AUDIO/harvard.wav) in 36.71 seconds\n",
      "  - Duration: 18.36 seconds\n"
     ]
    }
   ],
   "source": [
    "results = []  # Create an empty list to store transcription results\n",
    "\n",
    "# Create the \"transcripts\" directory if it doesn't exist\n",
    "text_transcripts = 'text_transtripts'\n",
    "os.makedirs(text_transcripts, exist_ok=True)\n",
    "\n",
    "\n",
    "# Create the ouput data directory if it doesn't exist\n",
    "data_transcripts = 'data_transtripts'\n",
    "os.makedirs(data_transcripts, exist_ok=True)\n",
    "\n",
    "for i, wav_file in enumerate(wav_files):\n",
    "\n",
    "    # Define start_time for each loop\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Extract filename without extension\n",
    "    filename, _ = os.path.splitext(os.path.basename(wav_file))\n",
    "\n",
    "    result = pipe(\n",
    "        wav_file,\n",
    "        # attn_implementation     =   \"eager\",\n",
    "        # return_timestamps       =   \"word\",\n",
    "        generate_kwargs         =    {\"language\": \"dutch\"}\n",
    "    )\n",
    "\n",
    "    # show the amount of time it took to process the current audio file\n",
    "    processing_time = time.time() - start_time  # Calculate processing time\n",
    "    print(f\"Processed file {i+1} ({wav_file}) in {processing_time:.2f} seconds\")\n",
    "\n",
    "    # Get WAV duration using soundfile\n",
    "    audio_data, sample_rate = read(wav_file)\n",
    "    duration = len(audio_data) / sample_rate\n",
    "    print(f\"  - Duration: {duration:.2f} seconds\")  # Print duration\n",
    "\n",
    "\n",
    "    # retreive chuncked data + correct voor time data type int \n",
    "    data = create_dataframe(result[\"chunks\"])\n",
    "    data.iloc[:,0]=data.iloc[:,0].fillna(-1)\n",
    "    data.iloc[:,0]=data.iloc[:,0].astype(int, errors='ignore')\n",
    "    data.iloc[:,1]=data.iloc[:,1].fillna(-1)\n",
    "    data.iloc[:,1]=data.iloc[:,1].astype(int, errors='ignore')\n",
    "\n",
    "\n",
    "    # Save transcript with matching filename (excluding .wav) and .txt extension\n",
    "    with open(os.path.join(text_transcripts, f\"{filename}.txt\"), \"w\") as text_file:\n",
    "       text_file.write(result[\"text\"])\n",
    "\n",
    "    # Save tthe DataFrame to a CSV file with full path\n",
    "    data.to_excel(os.path.join(data_transcripts, f\"{filename}.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
